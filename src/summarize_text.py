"""
é•·æ–‡æœ¬ç¸½çµ Agent
æ”¯æŒå°‡é•·æ–‡æœ¬åˆ†å¡Šï¼Œå°æ¯å€‹å¡Šé€²è¡Œç¸½çµï¼Œæœ€å¾Œç”Ÿæˆæ•´é«”ç¸½çµ
æ”¯æŒç•°æ­¥ä¸¦ç™¼è™•ç†å’Œæ›´å¤§çš„æ–‡æœ¬å¡Š
"""
import os
import sys
import logging
from datetime import datetime
from typing import List, Optional
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# è™•ç†å°å…¥è·¯å¾‘
try:
    from .chat_completion import chat_completion_simple
except ImportError:
    # å¦‚æœç›¸å°å°å…¥å¤±æ•—ï¼Œå˜—è©¦çµ•å°å°å…¥
    sys.path.insert(0, str(Path(__file__).parent))
    from chat_completion import chat_completion_simple

# é…ç½®æ—¥å¿—
def setup_logger(log_file: Optional[str] = None):
    """
    é…ç½®æ—¥å¿—è®°å½•å™¨
    
    å‚æ•°:
        log_file (str, optional): æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™åªè¾“å‡ºåˆ°æ§åˆ¶å°
    """
    logger = logging.getLogger('summarize_text')
    logger.setLevel(logging.INFO)
    
    # é¿å…é‡å¤æ·»åŠ handler
    if logger.handlers:
        return logger
    
    # æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # æ§åˆ¶å°handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶handlerï¼ˆå¦‚æœæŒ‡å®šäº†æ—¥å¿—æ–‡ä»¶ï¼‰
    if log_file:
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger


def split_text_into_chunks(
    text: str,
    chunk_size: int = 100000,  # GPT-4o æ”¯æŒ 128k tokensï¼Œçº¦ç­‰äº 100k-150k å­—ç¬¦ï¼ˆä¸­æ–‡/è‹±æ–‡æ··åˆï¼‰
    chunk_overlap: int = 300  # ç›¸åº”å¢å¤§é‡å éƒ¨åˆ†ä»¥ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
) -> List[str]:
    """
    å°‡é•·æ–‡æœ¬åˆ†å¡Š
    
    åƒæ•¸:
        text (str): è¦åˆ†å¡Šçš„æ–‡æœ¬
        chunk_size (int): æ¯å¡Šçš„æœ€å¤§å­—ç¬¦æ•¸ï¼Œé»˜èªç‚º 100000ï¼ˆå……åˆ†åˆ©ç”¨ GPT-4o çš„ 128k tokens ä¸Šä¸‹æ–‡ï¼‰
        chunk_overlap (int): å¡Šä¹‹é–“çš„é‡ç–Šå­—ç¬¦æ•¸ï¼Œé»˜èªç‚º 5000
    
    è¿”å›:
        List[str]: æ–‡æœ¬å¡Šåˆ—è¡¨
    
    ç¤ºä¾‹:
        >>> text = "å¾ˆé•·çš„æ–‡æœ¬..."
        >>> chunks = split_text_into_chunks(text, chunk_size=1000)
        >>> print(f"åˆ†æˆ {len(chunks)} å¡Š")
    """
    if not text:
        return []
    
    text_length = len(text)
    
    # å¦‚æœæ–‡æœ¬é•·åº¦å°æ–¼å¡Šå¤§å°ï¼Œç›´æ¥è¿”å›æ•´å€‹æ–‡æœ¬ä½œç‚ºä¸€å€‹å¡Š
    if text_length <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    # ç¢ºä¿æ¯æ¬¡è‡³å°‘è™•ç†ä¸€å®šæ•¸é‡çš„å­—ç¬¦ï¼Œé¿å…ç”¢ç”Ÿéå¤šå°å¡Š
    min_chunk_size = max(100, chunk_size // 100)  # è‡³å°‘100å­—ç¬¦æˆ–chunk_sizeçš„1%
    
    while start < text_length:
        # è¨ˆç®—ç•¶å‰å¡Šçš„çµæŸä½ç½®
        end = min(start + chunk_size, text_length)
        last_end = end  # è¨˜éŒ„åŸå§‹çµæŸä½ç½®
        
        # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€å¡Šï¼Œå˜—è©¦åœ¨å¥è™Ÿã€æ›è¡Œç¬¦ç­‰ä½ç½®åˆ‡æ–·
        if end < text_length:
            # å°‹æ‰¾åˆé©çš„åˆ†å‰²é»ï¼ˆå„ªå…ˆé¸æ“‡å¥è™Ÿã€å•è™Ÿã€æ„Ÿå˜†è™Ÿã€æ›è¡Œç¬¦ï¼‰
            for separator in ['ã€‚\n', 'ã€‚ ', '\n\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', '\n']:
                last_sep = text.rfind(separator, start, end)
                if last_sep != -1:
                    # ç¢ºä¿åˆ†å‰²é»ä¸æœƒå°è‡´å¡Šå¤ªå°
                    potential_end = last_sep + len(separator)
                    if potential_end - start >= min_chunk_size:
                        end = potential_end
                        break
        
        # å¦‚æœå‰©é¤˜æ–‡æœ¬ä¸è¶³æœ€å°å¡Šå¤§å°ï¼Œç›´æ¥å–åˆ°æœ«å°¾
        if text_length - start < min_chunk_size:
            end = text_length
        
        # æå–ç•¶å‰å¡Š
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        # è¨ˆç®—ä¸‹ä¸€å€‹å¡Šçš„èµ·å§‹ä½ç½®ï¼ˆè€ƒæ…®é‡ç–Šï¼‰
        # ç¢ºä¿è‡³å°‘å‘å‰ç§»å‹•ä¸€å®šè·é›¢ï¼Œé¿å…é™·å…¥ç„¡é™å¾ªç’°
        next_start = max(end - chunk_overlap, start + min(1000, chunk_size // 10))
        prev_start = start  # è¨˜éŒ„å‰ä¸€å€‹startä½ç½®
        start = min(next_start, text_length)  # ç¢ºä¿ä¸è¶…éæ–‡æœ¬é•·åº¦
        
        # é˜²æ­¢æ­»å¾ªç’°ï¼šå¦‚æœstartæ²’æœ‰è¶³å¤ å‰é€²ï¼Œå¼·åˆ¶å‰é€²åˆ°endä½ç½®
        if start <= prev_start:
            start = end
            if start >= text_length:
                break
    
    return chunks


def summarize_chunk(
    chunk: str,
    chunk_index: int,
    total_chunks: int,
    api_key: str,
    model: str = "chatgpt-4o-latest",
    language: str = "ä¸­æ–‡",
    logger: Optional[logging.Logger] = None
) -> str:
    """
    ç¸½çµå–®å€‹æ–‡æœ¬å¡Š
    
    åƒæ•¸:
        chunk (str): è¦ç¸½çµçš„æ–‡æœ¬å¡Š
        chunk_index (int): ç•¶å‰å¡Šçš„ç´¢å¼•ï¼ˆå¾ 1 é–‹å§‹ï¼‰
        total_chunks (int): ç¸½å¡Šæ•¸
        api_key (str): API å¯†é‘°
        model (str): æ¨¡å‹åç¨±
        language (str): ç¸½çµä½¿ç”¨çš„èªè¨€ï¼Œé»˜èªç‚º "ä¸­æ–‡"
        logger (logging.Logger, optional): æ—¥å¿—è®°å½•å™¨
    
    è¿”å›:
        str: è©²å¡Šçš„ç¸½çµ
    """
    if logger:
        logger.info(f"é–‹å§‹ç¸½çµç¬¬ {chunk_index}/{total_chunks} å¡Šï¼ˆé•·åº¦: {len(chunk)} å­—ç¬¦ï¼‰")
    
    system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬ç¸½çµåŠ©æ‰‹ã€‚ä½ çš„ä»»å‹™æ˜¯å°çµ¦å®šçš„æ–‡æœ¬é€²è¡Œæ·±å…¥åˆ†æï¼Œæå–ä¸¦ç¸½çµæ ¸å¿ƒè§€é»å’Œè«–è¿°ã€‚
è¦æ±‚ï¼š
1. é‡é»ç¸½çµæ–‡æœ¬ä¸­çš„æ ¸å¿ƒè§€é»ã€è«–è­‰å’Œä¸»å¼µ
2. æä¾›å…·é«”çš„è«–è­‰éç¨‹ã€æ¡ˆä¾‹å’Œæ•¸æ“šæ”¯æŒï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
3. ä½¿ç”¨{language}é€²è¡Œç¸½çµ
4. æ¡ç”¨åˆ†æ®µå±•ç¤ºçš„æ–¹å¼ï¼Œæ¯å€‹è§€é»æˆ–è«–è¿°ä½¿ç”¨ç¨ç«‹æ®µè½
5. ä¿æŒé‚è¼¯æ¸…æ™°ï¼Œçµæ§‹å®Œæ•´ï¼Œé¿å…éæ–¼ç°¡åŒ–
6. å¦‚æœæ–‡æœ¬æ¶‰åŠç‰¹å®šé ˜åŸŸï¼ˆå¦‚æŠ€è¡“ã€ç§‘å­¸ã€å•†æ¥­ç­‰ï¼‰ï¼Œè«‹ä¿æŒå°ˆæ¥­æ€§å’Œæº–ç¢ºæ€§"""
    
    prompt = f"""è«‹å°ä»¥ä¸‹æ–‡æœ¬ï¼ˆç¬¬ {chunk_index}/{total_chunks} å¡Šï¼‰é€²è¡Œæ·±å…¥ç¸½çµï¼Œé‡é»é—œæ³¨è§€é»å’Œè«–è¿°ï¼š

{chunk}

è«‹æŒ‰ç…§ä»¥ä¸‹è¦æ±‚æä¾›ç¸½çµï¼š
1. æå–æ–‡æœ¬ä¸­çš„æ ¸å¿ƒè§€é»å’Œä¸»è¦è«–è¿°
2. æä¾›å…·é«”çš„è«–è­‰éç¨‹ã€æ¡ˆä¾‹ã€æ•¸æ“šæˆ–ä¾‹è­‰ï¼ˆå¦‚æ–‡æœ¬ä¸­åŒ…å«ï¼‰
3. ä½¿ç”¨åˆ†æ®µå±•ç¤ºï¼Œæ¯å€‹ä¸»è¦è§€é»æˆ–è«–è¿°å–®ç¨æˆæ®µ
4. ä¿æŒå…§å®¹å…·é«”ï¼Œé¿å…éæ–¼æŠ½è±¡æˆ–æ¦‚æ‹¬
5. ç¢ºä¿é‚è¼¯é€£è²«ï¼Œè§€é»æ¸…æ™°

è«‹é–‹å§‹ç¸½çµï¼š"""
    
    try:
        summary = chat_completion_simple(
            prompt=prompt,
            api_key=api_key,
            model=model,
            system_prompt=system_prompt,
            temperature=0.3,  # è¼ƒä½çš„æº«åº¦ä»¥ä¿è­‰ç¸½çµçš„ä¸€è‡´æ€§å’Œæº–ç¢ºæ€§
            max_tokens=8000  # å¢å¤§è¾“å‡º token é™åˆ¶ï¼Œå……åˆ†åˆ©ç”¨ GPT-4o çš„èƒ½åŠ›
        )
        if logger:
            logger.info(f"æˆåŠŸå®Œæˆç¬¬ {chunk_index}/{total_chunks} å¡Šçš„ç¸½çµï¼ˆç¸½çµé•·åº¦: {len(summary)} å­—ç¬¦ï¼‰")
        return summary
    except Exception as e:
        error_msg = f"ç¸½çµç¬¬ {chunk_index} å¡Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        if logger:
            logger.error(error_msg, exc_info=True)
        print(f"âš ï¸ {error_msg}")
        return f"[ç¸½çµå¤±æ•—: {str(e)}]"


def summarize_text(
    text: str,
    api_key: Optional[str] = None,
    model: str = "chatgpt-4o-latest",
    chunk_size: int = 100000,  # GPT-4o æ”¯æŒ 128k tokensï¼Œçº¦ç­‰äº 100k-150k å­—ç¬¦
    chunk_overlap: int = 300,  # ç›¸åº”å¢å¤§é‡å ä»¥ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
    language: str = "ä¸­æ–‡",
    show_progress: bool = True,
    enable_async: bool = True,
    max_workers: int = 5,  # å¹¶å‘æ€»ç»“çš„çº¿ç¨‹æ•°
    save_chunk_summaries: bool = True,  # æ˜¯å¦ä¿å­˜åˆ†å—æ€»ç»“
    output_dir: Optional[str] = None  # è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤ç›®å½•
) -> str:
    """
    ç¸½çµé•·æ–‡æœ¬çš„ä¸»å‡½æ•¸
    
    åƒæ•¸:
        text (str): è¦ç¸½çµçš„é•·æ–‡æœ¬
        api_key (str, optional): API å¯†é‘°ï¼Œå¦‚æœç‚º None å‰‡å¾ç’°å¢ƒè®Šé‡è®€å–
        model (str): æ¨¡å‹åç¨±ï¼Œé»˜èªç‚º "chatgpt-4o-latest"
        chunk_size (int): æ¯å¡Šçš„æœ€å¤§å­—ç¬¦æ•¸ï¼Œé»˜èªç‚º 100000ï¼ˆå……åˆ†åˆ©ç”¨ GPT-4o çš„ 128k tokens ä¸Šä¸‹æ–‡ï¼‰
        chunk_overlap (int): å¡Šä¹‹é–“çš„é‡ç–Šå­—ç¬¦æ•¸ï¼Œé»˜èªç‚º 5000
        language (str): ç¸½çµä½¿ç”¨çš„èªè¨€ï¼Œé»˜èªç‚º "ä¸­æ–‡"
        show_progress (bool): æ˜¯å¦é¡¯ç¤ºé€²åº¦ï¼Œé»˜èªç‚º True
        enable_async (bool): æ˜¯å¦å•Ÿç”¨ç•°æ­¥ä¸¦ç™¼ç¸½çµï¼Œé»˜èªç‚º True
        max_workers (int): ä¸¦ç™¼ç¸½çµçš„æœ€å¤§ç·šç¨‹æ•¸ï¼Œé»˜èªç‚º 5
        save_chunk_summaries (bool): æ˜¯å¦ä¿å­˜åˆ†å—æ€»ç»“åˆ°txtæ–‡ä»¶ï¼Œé»˜è®¤ä¸º True
        output_dir (str, optional): è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤çš„outputsç›®å½•
    
    è¿”å›:
        str: æœ€çµ‚çš„æ–‡æœ¬ç¸½çµ
    
    ç¤ºä¾‹:
        >>> long_text = "å¾ˆé•·çš„æ–‡æœ¬å…§å®¹..."
        >>> summary = summarize_text(long_text, api_key="your-api-key")
        >>> print(summary)
    """
    # è¨­ç½®æ—¥å¿—
    if output_dir is None:
        output_dir = Path(__file__).parent.parent / "outputs"
    else:
        output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = output_dir / f"summarize_{timestamp}.log"
    logger = setup_logger(str(log_file))
    
    logger.info("=" * 60)
    logger.info("é–‹å§‹é•·æ–‡æœ¬ç¸½çµä»»å‹™")
    logger.info("=" * 60)
    logger.info(f"æ–‡æœ¬é•·åº¦: {len(text)} å­—ç¬¦")
    logger.info(f"æ¨¡å‹: {model}")
    logger.info(f"å¡Šå¤§å°: {chunk_size}, é‡ç–Š: {chunk_overlap}")
    logger.info(f"èªè¨€: {language}")
    logger.info(f"ä¸¦ç™¼è™•ç†: {enable_async}, æœ€å¤§ç·šç¨‹æ•¸: {max_workers}")
    
    # ç²å– API key
    if api_key is None:
        api_key = os.getenv("API_KEY_302_AI") or os.getenv("OPENAI_API_KEY")
        if not api_key:
            error_msg = "è«‹æä¾› API å¯†é‘°æˆ–è¨­ç½®ç’°å¢ƒè®Šé‡ API_KEY_302_AI æˆ– OPENAI_API_KEY"
            logger.error(error_msg)
            raise ValueError(error_msg)
    
    if not text or not text.strip():
        error_msg = "æ–‡æœ¬ä¸èƒ½ç‚ºç©º"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    # æ­¥é©Ÿ 1: å°‡æ–‡æœ¬åˆ†å¡Š
    logger.info("æ­¥é©Ÿ 1: é–‹å§‹å°‡æ–‡æœ¬åˆ†å¡Š")
    if show_progress:
        print(f"ğŸ“ æ­£åœ¨å°‡æ–‡æœ¬åˆ†å¡Šï¼ˆå¡Šå¤§å°: {chunk_size}, é‡ç–Š: {chunk_overlap}ï¼‰...")
    
    chunks = split_text_into_chunks(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    
    if not chunks:
        error_msg = "æ–‡æœ¬åˆ†å¡Šå¤±æ•—ï¼Œæœªç”Ÿæˆä»»ä½•å¡Š"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    total_chunks = len(chunks)
    logger.info(f"æ–‡æœ¬å·²åˆ†æˆ {total_chunks} å¡Š")
    for i, chunk in enumerate(chunks, 1):
        logger.info(f"  å¡Š {i}: {len(chunk)} å­—ç¬¦")
    
    if show_progress:
        print(f"âœ“ æ–‡æœ¬å·²åˆ†æˆ {total_chunks} å¡Š\n")
    
    # å¦‚æœåªæœ‰ä¸€å¡Šï¼Œç›´æ¥ç¸½çµ
    if total_chunks == 1:
        logger.info("æ–‡æœ¬åªæœ‰ä¸€å¡Šï¼Œç›´æ¥é€²è¡Œç¸½çµ")
        if show_progress:
            print("ğŸ“Š æ–‡æœ¬è¼ƒçŸ­ï¼Œç›´æ¥é€²è¡Œç¸½çµ...")
        summary = summarize_chunk(
            chunks[0],
            chunk_index=1,
            total_chunks=1,
            api_key=api_key,
            model=model,
            language=language,
            logger=logger
        )
        # ä¿å­˜åˆ†å—æ€»ç»“
        if save_chunk_summaries:
            chunk_summary_file = output_dir / f"chunk_summaries_{timestamp}.txt"
            with open(chunk_summary_file, 'w', encoding='utf-8') as f:
                f.write("=" * 60 + "\n")
                f.write("åˆ†å¡Šç¸½çµï¼ˆæŒ‰é †åºï¼‰\n")
                f.write("=" * 60 + "\n\n")
                f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ç¸½å¡Šæ•¸: 1\n\n")
                f.write("=" * 60 + "\n")
                f.write("ç¬¬ 1 å¡Šç¸½çµ:\n")
                f.write("=" * 60 + "\n\n")
                f.write(summary)
            logger.info(f"åˆ†å¡Šç¸½çµå·²ä¿å­˜åˆ°: {chunk_summary_file}")
            if show_progress:
                print(f"ğŸ’¾ åˆ†å¡Šç¸½çµå·²ä¿å­˜åˆ°: {chunk_summary_file}")
        return summary
    
    # æ­¥é©Ÿ 2: å°æ¯å€‹å¡Šé€²è¡Œç¸½çµï¼ˆæ”¯æŒä¸¦ç™¼ï¼‰
    logger.info("æ­¥é©Ÿ 2: é–‹å§‹å°å„å€‹æ–‡æœ¬å¡Šé€²è¡Œç¸½çµ")
    if show_progress:
        if enable_async:
            print(f"ğŸ“‹ é–‹å§‹ä¸¦ç™¼ç¸½çµå„å€‹æ–‡æœ¬å¡Šï¼ˆæœ€å¤§ {max_workers} å€‹ç·šç¨‹ï¼‰...\n")
        else:
            print(f"ğŸ“‹ é–‹å§‹ç¸½çµå„å€‹æ–‡æœ¬å¡Š...\n")
    
    chunk_summaries = []
    
    if enable_async and total_chunks > 1:
        # ä½¿ç”¨ç·šç¨‹æ± ä¸¦ç™¼ç¸½çµ
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_chunk = {}
            
            for i, chunk in enumerate(chunks, start=1):
                future = executor.submit(
                    summarize_chunk,
                    chunk,
                    chunk_index=i,
                    total_chunks=total_chunks,
                    api_key=api_key,
                    model=model,
                    language=language,
                    logger=logger
                )
                future_to_chunk[future] = i
            
            # æ”¶é›†çµæœï¼ˆæŒ‰é †åºï¼‰
            completed = 0
            results_dict = {}  # ä½¿ç”¨å­—å…¸ä¿å­˜çµæœï¼Œä»¥ä¿æŒé †åº
            
            for future in as_completed(future_to_chunk):
                chunk_idx = future_to_chunk[future]
                try:
                    summary = future.result()
                    results_dict[chunk_idx] = summary
                    completed += 1
                    
                    logger.info(f"å®Œæˆç¬¬ {chunk_idx}/{total_chunks} å¡Šçš„ç¸½çµ")
                    if show_progress:
                        print(f"  âœ“ å®Œæˆç¬¬ {chunk_idx}/{total_chunks} å¡Š ({completed}/{total_chunks})")
                except Exception as e:
                    logger.error(f"ç¸½çµç¬¬ {chunk_idx} å¡Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
                    print(f"  âš ï¸ ç¸½çµç¬¬ {chunk_idx} å¡Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    results_dict[chunk_idx] = f"[ç¸½çµå¤±æ•—: {str(e)}]"
            
            # æŒ‰é †åºçµ„è£çµæœ
            chunk_summaries = [results_dict[i] for i in range(1, total_chunks + 1) if i in results_dict]
            logger.info(f"æ‰€æœ‰ {len(chunk_summaries)} å€‹åˆ†å¡Šç¸½çµå·²å®Œæˆ")
    else:
        # é †åºè™•ç†
        for i, chunk in enumerate(chunks, start=1):
            if show_progress:
                print(f"  è™•ç†ç¬¬ {i}/{total_chunks} å¡Š...", end=" ", flush=True)
            
            summary = summarize_chunk(
                chunk,
                chunk_index=i,
                total_chunks=total_chunks,
                api_key=api_key,
                model=model,
                language=language,
                logger=logger
            )
            
            chunk_summaries.append(summary)
            logger.info(f"å®Œæˆç¬¬ {i}/{total_chunks} å¡Šçš„ç¸½çµ")
            
            if show_progress:
                print("âœ“")
        
        logger.info(f"æ‰€æœ‰ {len(chunk_summaries)} å€‹åˆ†å¡Šç¸½çµå·²å®Œæˆ")
    
    # ä¿å­˜åˆ†å—æ€»ç»“åˆ°txtæ–‡ä»¶ï¼ˆæŒ‰é¡ºåºï¼‰
    if save_chunk_summaries:
        chunk_summary_file = output_dir / f"chunk_summaries_{timestamp}.txt"
        logger.info(f"æ­£åœ¨ä¿å­˜åˆ†å¡Šç¸½çµåˆ°æ–‡ä»¶: {chunk_summary_file}")
        try:
            with open(chunk_summary_file, 'w', encoding='utf-8') as f:
                f.write("=" * 60 + "\n")
                f.write("åˆ†å¡Šç¸½çµï¼ˆæŒ‰é †åºï¼‰\n")
                f.write("=" * 60 + "\n\n")
                f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ç¸½å¡Šæ•¸: {total_chunks}\n")
                f.write(f"æ¨¡å‹: {model}\n")
                f.write(f"èªè¨€: {language}\n\n")
                f.write("=" * 60 + "\n\n")
                
                for i, summary in enumerate(chunk_summaries, 1):
                    f.write("=" * 60 + "\n")
                    f.write(f"ç¬¬ {i} å¡Šç¸½çµ:\n")
                    f.write("=" * 60 + "\n\n")
                    f.write(summary)
                    f.write("\n\n")
            
            logger.info(f"åˆ†å¡Šç¸½çµå·²æˆåŠŸä¿å­˜åˆ°: {chunk_summary_file}")
            if show_progress:
                print(f"\nğŸ’¾ åˆ†å¡Šç¸½çµå·²ä¿å­˜åˆ°: {chunk_summary_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†å¡Šç¸½çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            print(f"âš ï¸ ä¿å­˜åˆ†å¡Šç¸½çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    # æ­¥é©Ÿ 3: åˆä½µæ‰€æœ‰å¡Šçš„ç¸½çµï¼Œç”Ÿæˆæœ€çµ‚ç¸½çµ
    logger.info("æ­¥é©Ÿ 3: é–‹å§‹ç”Ÿæˆæœ€çµ‚ç¸½çµ")
    if show_progress:
        print(f"\nğŸ“‘ æ­£åœ¨ç”Ÿæˆæœ€çµ‚ç¸½çµ...")
    
    # åˆä½µæ‰€æœ‰å¡Šçš„ç¸½çµ
    combined_summaries = "\n\n".join([
        f"ç¬¬ {i+1} å¡Šç¸½çµï¼š\n{summary}"
        for i, summary in enumerate(chunk_summaries)
    ])
    
    system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬ç¸½çµåŠ©æ‰‹ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šå¤šå€‹æ–‡æœ¬å¡Šçš„ç¸½çµï¼Œç”Ÿæˆä¸€å€‹å®Œæ•´ã€é€£è²«ã€å…·é«”çš„æ•´é«”ç¸½çµã€‚
è¦æ±‚ï¼š
1. æ•´åˆæ‰€æœ‰å¡Šçš„ç¸½çµï¼Œå½¢æˆä¸€å€‹é‚è¼¯æ¸…æ™°çš„æ•´é«”ç¸½çµ
2. é‡é»ç¸½çµæ–‡æœ¬çš„æ ¸å¿ƒè§€é»ã€è«–è­‰å’Œä¸»å¼µ
3. æä¾›å…·é«”çš„è«–è­‰éç¨‹ã€æ¡ˆä¾‹ã€æ•¸æ“šæˆ–ä¾‹è­‰
4. ä½¿ç”¨åˆ†æ®µå±•ç¤ºçš„æ–¹å¼ï¼Œæ¯å€‹ä¸»è¦è§€é»æˆ–è«–è¿°ä½¿ç”¨ç¨ç«‹æ®µè½
5. æ¶ˆé™¤é‡è¤‡ä¿¡æ¯ï¼Œä½†ä¿ç•™é‡è¦çš„è§€é»ç´°ç¯€
6. ä¿æŒç¸½çµçš„å®Œæ•´æ€§å’Œé€£è²«æ€§
7. ä½¿ç”¨{language}é€²è¡Œç¸½çµ
8. ç¢ºä¿ç¸½çµèƒ½å¤ å…¨é¢ã€å…·é«”åœ°åæ˜ åŸæ–‡çš„æ ¸å¿ƒå…§å®¹å’Œä¸»è¦è§€é»"""
    
    final_prompt = f"""ä»¥ä¸‹æ˜¯å°é•·æ–‡æœ¬å„å€‹éƒ¨åˆ†çš„ç¸½çµï¼š

{combined_summaries}

è«‹æ ¹æ“šä»¥ä¸Šå„å€‹éƒ¨åˆ†çš„ç¸½çµï¼Œç”Ÿæˆä¸€å€‹å®Œæ•´ã€é€£è²«ã€å…·é«”çš„æ•´é«”ç¸½çµã€‚è«‹æŒ‰ç…§ä»¥ä¸‹è¦æ±‚ï¼š
1. æ•´åˆæ‰€æœ‰é—œéµä¿¡æ¯å’Œè§€é»ï¼Œå½¢æˆé‚è¼¯æ¸…æ™°çš„ç¸½çµ
2. é‡é»çªå‡ºæ ¸å¿ƒè§€é»å’Œä¸»è¦è«–è¿°ï¼Œæä¾›å…·é«”çš„è«–è­‰éç¨‹
3. å¦‚æœå„éƒ¨åˆ†ç¸½çµä¸­åŒ…å«æ¡ˆä¾‹ã€æ•¸æ“šæˆ–ä¾‹è­‰ï¼Œè«‹åœ¨æœ€çµ‚ç¸½çµä¸­ä¿ç•™
4. ä½¿ç”¨åˆ†æ®µå±•ç¤ºï¼Œæ¯å€‹ä¸»è¦è§€é»æˆ–è«–è¿°å–®ç¨æˆæ®µï¼Œçµæ§‹æ¸…æ™°
5. æ¶ˆé™¤é‡è¤‡å…§å®¹ï¼Œä½†ä¿ç•™è§€é»çš„å…·é«”ç´°ç¯€å’Œè«–è­‰
6. ä¿æŒå…§å®¹å…·é«”ï¼Œé¿å…éæ–¼æŠ½è±¡æˆ–æ¦‚æ‹¬
7. ç¢ºä¿çµæ§‹å®Œæ•´ï¼Œèªè¨€æµæš¢ï¼Œè§€é»æ¸…æ™°

è«‹ç”Ÿæˆä¸€å€‹åˆ†æ®µå±•ç¤ºçš„è©³ç´°ç¸½çµï¼š"""
    
    try:
        # å……åˆ†åˆ©ç”¨ GPT-4o çš„ 128k tokens ä¸Šä¸‹æ–‡ï¼Œå¢å¤§ max_tokens è¾“å‡ºé™åˆ¶
        logger.info("èª¿ç”¨APIç”Ÿæˆæœ€çµ‚ç¸½çµ")
        final_summary = chat_completion_simple(
            prompt=final_prompt,
            api_key=api_key,
            model=model,
            system_prompt=system_prompt,
            temperature=0.3,
            max_tokens=16000  # å¢å¤§ä»¥å……åˆ†åˆ©ç”¨ GPT-4o çš„èƒ½åŠ›ç”Ÿæˆæ›´è¯¦ç»†çš„æ€»ç»“
        )
        
        logger.info(f"æœ€çµ‚ç¸½çµç”ŸæˆæˆåŠŸï¼ˆé•·åº¦: {len(final_summary)} å­—ç¬¦ï¼‰")
        logger.info("=" * 60)
        logger.info("é•·æ–‡æœ¬ç¸½çµä»»å‹™å®Œæˆ")
        logger.info("=" * 60)
        
        if show_progress:
            print("âœ“ ç¸½çµå®Œæˆï¼\n")
        
        return final_summary
    except Exception as e:
        error_msg = f"ç”Ÿæˆæœ€çµ‚ç¸½çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        logger.error(error_msg, exc_info=True)
        raise Exception(error_msg)


if __name__ == "__main__":
    import sys
    
    # å˜—è©¦åŠ è¼‰.envæ–‡ä»¶
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except ImportError:
        pass
    
    print("=" * 60)
    print("é•·æ–‡æœ¬ç¸½çµ Agent")
    print("=" * 60)
    
    # å¾ç’°å¢ƒè®Šé‡ç²å– API key
    api_key = os.getenv("API_KEY_302_AI") or os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        print("\nâŒ éŒ¯èª¤: è«‹è¨­ç½®ç’°å¢ƒè®Šé‡ API_KEY_302_AI æˆ– OPENAI_API_KEY")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  export API_KEY_302_AI='your-api-key'")
        print("  python summarize_text.py <æ–‡æœ¬æ–‡ä»¶è·¯å¾‘>")
        sys.exit(1)
    
    if len(sys.argv) < 2:
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python summarize_text.py <æ–‡æœ¬æ–‡ä»¶è·¯å¾‘> [å¡Šå¤§å°] [æ¨¡å‹åç¨±]")
        print("\nç¤ºä¾‹:")
        print("  python summarize_text.py document.txt")
        print("  python summarize_text.py document.txt 2000 chatgpt-4o-latest")
        sys.exit(1)
    
    file_path = sys.argv[1]
    chunk_size = int(sys.argv[2]) if len(sys.argv) > 2 else 2000
    model = sys.argv[3] if len(sys.argv) > 3 else "chatgpt-4o-latest"
    
    try:
        # è®€å–æ–‡æœ¬æ–‡ä»¶
        print(f"\nğŸ“– è®€å–æ–‡ä»¶: {file_path}")
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        if not text.strip():
            print("âŒ éŒ¯èª¤: æ–‡ä»¶ç‚ºç©º")
            sys.exit(1)
        
        print(f"âœ“ æ–‡ä»¶é•·åº¦: {len(text)} å­—ç¬¦\n")
        
        # åŸ·è¡Œç¸½çµ
        summary = summarize_text(
            text=text,
            api_key=api_key,
            model=model,
            chunk_size=chunk_size,
            show_progress=True
        )
        
        print("=" * 60)
        print("æœ€çµ‚ç¸½çµ:")
        print("=" * 60)
        print(summary)
        print("\n" + "=" * 60)
        
        # å¯é¸ï¼šä¿å­˜ç¸½çµåˆ°æ–‡ä»¶
        output_file = file_path.rsplit('.', 1)[0] + "_summary.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("é•·æ–‡æœ¬ç¸½çµ\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"åŸæ–‡æ–‡ä»¶: {file_path}\n")
            f.write(f"åŸæ–‡é•·åº¦: {len(text)} å­—ç¬¦\n\n")
            f.write("=" * 60 + "\n")
            f.write("ç¸½çµ:\n")
            f.write("=" * 60 + "\n\n")
            f.write(summary)
        
        print(f"\nğŸ’¾ ç¸½çµå·²ä¿å­˜åˆ°: {output_file}")
        
    except FileNotFoundError:
        print(f"\nâŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æ–‡ä»¶ '{file_path}'")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

